import argparse
import os
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
import plotnine as p9


def process_vasttools(file, allowed_nas, groups):
    """
   Reads vastools tidy output file (which already has PSI
   inclusion levels per event) and performs PCA analysis
   based on the samples
   :param str file: Vast-tools file generated by tidy utility
   :param int allowed_nas: Number of samples with NAs (unquantifiable
   PSIs) per event allowed, so that event is kept for PCA analysis
   :param dict groups: Dictionary with info about each sample
    and the group they represent
    """
    df = pd.read_csv(file, sep="\t", index_col=0)
    assert all(n in df.columns for n in list(groups.keys())), "All samples in the groups file must exist in tidy" \
                                                              "file"
    df = df[list(groups.keys())]
    return df.dropna(axis=0, thresh=len(df.columns) - allowed_nas).apply(pd.to_numeric, errors='coerce')


def process_rMATS(files, allowed_nas, groups, paired, idx_to_use, outbasename):
    """
    Reads rMATS output files, extracts PSI inclusion levels
    assuming the sample order represented in the `groups` dict,
    and performs PCA analysis based on the samples
    :param list files: List of rMATS files
    :param int allowed_nas: Number of samples with NAs (unquantifiable
    PSIs) per event allowed, so that event is kept for PCA analysis
    :param dict groups: Dictionary with info about each sample
    and the group they represent
    :param bool paired: Whether samples of each group are paired (if so,
    order of samples per group must be preserved)
    :param str idx_to_use: Sample indexes to subset from each rMATS file
    :param str: Output basename
    """
    samples_names = list(groups.keys())
    if idx_to_use is not None:
        idx = idx_to_use.split(",")
        try:
            idx = [int(x) for x in idx]
        except ValueError:
            raise ValueError('Please set int values in the "--idx_to_use" argument')
        assert len(samples_names) == len(idx), "Number of samples in the groups({}) must match number of " \
                                               "indexes given({})".format(len(samples_names), len(idx))
    inc_level_col = "IncLevel1"
    events = ["SE", "MXE", "A3SS", "A5SS", "RI"]
    event_type_map = {"SE": "ES"}
    results_dict = {}

    with open(files) as f:
        file_list = f.read().splitlines()
    f.close()

    for ev in events:
        print("Processing {} events ".format(event_type_map.get(ev, ev)))
        ev_file = [e for e in file_list if ev in os.path.basename(e)][0]
        ev_dict = {}
        assert os.path.isfile(ev_file), "{} is not a valid file".format(ev_file)
        f = open(ev_file)
        header = f.readline().split("\t")
        assert inc_level_col in header, "IncLevel1 column (PSI quantification) should be present in " \
                                        "rMATS {} file.".format(ev_file)
        idx_target_column = header.index(inc_level_col)
        next(f)
        for line in f:
            l = line.split("\t")
            psi = l[idx_target_column]
            if idx_to_use is not None:
                _psi_val = psi.split(",")
                try:
                    psi = ','.join([_psi_val[i] for i in idx])
                except IndexError:
                    raise IndexError('Some indexes provided are greater than number of samples({})'.
                                     format(len(_psi_val)))

            if any(v for v in psi.split(",") if v != "NA"): # Keep only events with at least 1 non NA
                ev_dict["{}_{}".format(event_type_map.get(ev, ev), l[0])] = psi
        f.close()
        results_dict[ev] = ev_dict

    list_dfs = []
    for ev_type, psi_dict in results_dict.items():
        df = pd.DataFrame.from_dict(psi_dict, orient="index", columns=["psis"])
        df[samples_names] = df['psis'].str.split(",", expand=True).replace("NA", np.nan)
        del df['psis']
        df = df.dropna(axis=0, thresh=len(df.columns) - allowed_nas).apply(pd.to_numeric, errors='coerce')

        list_dfs.append(df)
        do_PCA(df, allowed_nas, groups, paired, outbasename, ev_type, "rMATS")
    concat_df = pd.concat(list_dfs)
    return concat_df


def _compute_LSV_junction_variance(row):
    """
    Extracts index of the junction with the greatest
    variance
    :param pd.Series row: PSI quantification for a given LSV
    :return:
    """
    junctions = row.str.split(";", expand=True).apply(pd.to_numeric, errors='coerce')
    idx_top_variance = junctions.var().idxmax()
    try:
        return junctions.iloc[:, idx_top_variance]
    except ValueError:
        # If not possible to calculate variance (e.g. Only 1 sample with no NaN values)
        return


def process_majiq(files, allowed_nas, groups):
    """
    Reads files generated by majiq psi, extracts the PSI
    for the best junction of a given LSV (highest variance
    across all the samples) and performs PCA analysis
    on the samples assuming the sample order represented
    in the `groups` dict.

    :param str files: File listing majiq psi files
    :param int allowed_nas: Number of samples with NAs (unquantifiable
    PSIs) per event allowed, so that event is kept for PCA analysis
    :param dict groups: Dictionary with info about each sample
    and the group they represent
    """

    psi_col = "E(PSI) per LSV junction"
    lsv_id_col = "LSV ID"
    psi_dict = {}
    with open(files) as f:
        file_list = f.read().splitlines()
    f.close()
    assert len(file_list) == len(groups.keys()), "Number of majiq files must be the same as the number" \
                                                 " of different samples represented in the 1st column" \
                                                 " of groups file."
    colnames = []
    list_samples_psis = []
    print("Files and sample match:")
    # Per sample dict of PSI for each LSV
    for i, file in enumerate(file_list):
        _df = pd.read_csv(file, sep="\t")
        _d = pd.Series(_df[psi_col].values, index=_df[lsv_id_col]).to_dict()
        list_samples_psis.append(_d)
        colnames.append(list(groups.keys())[i])
        print("{} {}".format(os.path.basename(file), list(groups.keys())[i]))

    # Join dicts into 1 single df
    for i, d in enumerate(list_samples_psis):
        for lsv_id, lsv_psis in d.items():
            if lsv_id in psi_dict.keys():
                _update = psi_dict[lsv_id]
                _update[i] = lsv_psis
                psi_dict[lsv_id] = _update
            else:
                _new_val = [None] * len(list_samples_psis)
                _new_val[i] = lsv_psis
                psi_dict[lsv_id] = _new_val

    df = pd.DataFrame.from_dict(psi_dict, orient="index", columns=colnames)
    df = df.dropna(axis=0, thresh=len(df.columns) - allowed_nas)
    print("Calculating top variance junction per LSV")
    return df.apply(_compute_LSV_junction_variance, axis=1).dropna(how='all')


def do_PCA(df, allowed_nas, groups, paired, outbasename, name, extra=None):
    """
    PCA analysis from PSIs quantifications based on the 2000 events with
    greatest variance. Additionally, plots of principal components based
    on the defined groups are drawn

    :param pd.DataFrame df: Ready df with per-event PSIs to calculate
    principal components
    :param int allowed_nas: Allowed NAs. If > 0, imputations will be
    performed based on the row mean (PCA doesn't accept missing values)
    :param dict groups: Dictionary with info about each sample
    and the group they represent
    :param bool paired: Whether samples of each group are paired (if so,
    order of samples per group must be preserved)
    :param str outbasename: Output basename
    :param str name: str to add to output (e.g. software name)
    :param str extra: Extra str to add (e.g. rMATS event type)
    :return:
    """
    print("Number of {} events that will be used in the PCA: {}".format(name, df.shape[0]))
    if allowed_nas > 0:
        print("Doing imputation of missing values")
        df = df.apply(lambda x: x.fillna(x.mean()), axis=1)

    df = df.loc[df.var(axis=1).nlargest(2000).index, ]
    pca = PCA(n_components=3)
    PCs = pca.fit_transform(df.T)
    print("Amount of variance that the first PCs contain for {} data: {}".format(name, pca.explained_variance_ratio_))

    PC_df = pd.DataFrame(data=PCs,
                         columns=['PC1', 'PC2', 'PC3'],
                         index=df.T.index)

    cols_groups_df = ['Group', 'Ind'] if paired else ['Group']
    groups_df = pd.DataFrame.from_dict(groups, orient='index', columns=cols_groups_df)

    PC_df = PC_df.merge(groups_df,
                        left_index=True,
                        right_index=True).rename_axis('Sample').reset_index()

    for pc_pair in [("PC1", "PC2"), ("PC2", "PC3")]:
        if paired:
            p1 = (p9.ggplot(PC_df, p9.aes(x=pc_pair[0], y=pc_pair[1], fill="Group", shape='Ind'))
                  + p9.geom_point(color="black", size=6, alpha=0.7, position=p9.position_dodge(width=0.3,
                                                                                                         preserve="total")))
        else:
            p1 = (p9.ggplot(PC_df, p9.aes(x=pc_pair[0], y=pc_pair[1], fill="Group", shape='Sample'))
                  + p9.geom_point(color="black", size=6, alpha=0.7, position=p9.position_dodge(width=0.3,
                                                                                                         preserve="total")))

        if extra is not None:
            output = "{}_{}_{}_{}vs{}.pdf".format(outbasename, extra, name, pc_pair[0], pc_pair[1])
        else:
            output = "{}_{}_{}vs{}.pdf".format(outbasename, name, pc_pair[0], pc_pair[1])
        p1.save(output, verbose=False)


def main():
    parser = argparse.ArgumentParser(description='Script to generate PCAs from inclusion levels of splicing events '
                                                 'quantified by different tools.')
    parser.add_argument(dest='tool', choices=("rmats", "majiq", "vastools"), help="Generate samples PCAs from "
                                                                                  "quantifications of given tool")
    parser.add_argument('--groups', required=True, help='2 (or 3) column file with the sample name (1st col) and group'
                                                        ' it belongs (2nd col). if tool == rmats", order of the samples'
                                                        ' in the file should be the same as provided to "--b1" when '
                                                        ' running rMATS. If tool == majiq, order of the samples'
                                                        ' should be the same as the order of files in the "majiq_files"'
                                                        ' argument. 3rd column, if set, should refer to the individual'
                                                        ' name when samples from different groups are paired (--paired'
                                                        ' flag set in the script).')
    parser.add_argument('--NAs_allowed', type=int, default=0, help="Number of samples per event allowed to have"
                                                                   " NA values. Default: 0")
    parser.add_argument('--paired', action='store_true', help="Whether samples in groups file are paired. If so, "
                                                              "3rd column in groups argument must be set providing "
                                                              "information about each individual")
    parser.add_argument('-o', '--outbasename', help='Basename to write output files')

    rmats = parser.add_argument_group('rMATS related arguments')
    rmats.add_argument('--rmats_files', help='Events files produced by rMATS ([ev_type].MATS.[JC|JCEC].txt, one per '
                                             'line. These files should represent a rMATS run where no statistical '
                                             'comparison was performed ("--statoff" and "--b2" not set)')
    rmats.add_argument('--idx_to_use', type=str, help="If samples passed in the groups are a subset of all samples"
                                                      " used in the '--b1' argument in rMATS, set the indexes in the"
                                                      " rMATS files (IncLevel column) that match those samples. "
                                                      " (Indexes start from 0 and are split split by ',')")

    vasttools = parser.add_argument_group('vast-tools related arguments')
    vasttools.add_argument('--tidy', help='Inclusion table produced by vast-tools tidy.')

    majiq = parser.add_argument_group('MAJIQ related arguments')
    majiq.add_argument('--majiq_files', help="LSV quantifications generated by MAJIQ PSI, one sample per line.")
    args = parser.parse_args()

    groups_dict = {}
    with open(args.groups) as f:
        for line in f:
            l = line.rstrip().split("\t")
            _sample = l[0]
            _grp = l[1]
            groups_dict[_sample] = [_grp, l[2]] if args.paired else [_grp]
    f.close()

    if args.tool == "rmats":
        assert args.rmats_files is not None, "When producing PCA from rMATS, please set its group arguments " \
                                             "accordingly. "
        df = process_rMATS(args.rmats_files, args.NAs_allowed, groups_dict, args.paired, args.idx_to_use,
                           args.outbasename)
        do_PCA(df, args.NAs_allowed, groups_dict, args.paired, args.outbasename, "all", "rMATS")

    elif args.tool == "vastools":
        assert args.tidy is not None, "When producing PCA from vast-tools, please set its group arguments accordingly."
        df = process_vasttools(args.tidy, args.NAs_allowed, groups_dict)
        do_PCA(df, args.NAs_allowed, groups_dict, args.paired, args.outbasename, "vast-tools")

    elif args.tool == "majiq":
        assert args.majiq_files is not None, "When producing PCA from MAJIQ, please set its group arguments " \
                                             "accordingly. "
        df = process_majiq(args.majiq_files, args.NAs_allowed, groups_dict)
        do_PCA(df, args.NAs_allowed, groups_dict, args.paired, args.outbasename, "majiq")


if __name__ == "__main__":
    main()
