import argparse
import os
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from process_voila_tsv import get_relevant_type
import plotnine as p9
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns


def process_vasttools(file: str, allowed_nas: int, groups: dict) -> pd.DataFrame:
    """
    Reads vastools tidy output file (which already has PSI
    inclusion levels per event) and performs PCA analysis
    based on the samples
    :param str file: Vast-tools file generated by tidy utility
    :param int allowed_nas: Number of samples with NAs (unquantifiable
    PSIs) per event allowed, so that event is kept for PCA analysis
    :param dict groups: Dictionary with info about each sample
    and the group they represent

    :return pd.DataFrame: Vastools events that pass the NAs filter
    """
    df = pd.read_csv(file, sep="\t", index_col=0)
    assert all(n in df.columns for n in list(groups.keys())), "All samples in the groups file must exist in tidy" \
                                                              "file"
    df = df[list(groups.keys())]
    return df.dropna(axis=0, thresh=len(df.columns) - allowed_nas).apply(pd.to_numeric, errors='coerce')


def process_rMATS(files: list, allowed_nas: int, groups: dict, 
                  paired: bool, idx_to_use: str, outbasename: str, 
                  n_to_show_names: int) -> pd.DataFrame:
    """
    Reads rMATS output files, extracts PSI inclusion levels
    assuming the sample order represented in the `groups` dict,
    and performs PCA analysis based on the samples
    :param list files: List of rMATS files
    :param int allowed_nas: Number of samples with NAs (unquantifiable
    PSIs) per event allowed, so that event is kept for PCA analysis
    :param dict groups: Dictionary with info about each sample
    and the group they represent
    :param bool paired: Whether samples of each group are paired (if so,
    order of samples per group must be preserved)
    :param str idx_to_use: Sample indexes to subset from each rMATS file
    :param str outbasename: Output basename
    :param int n_to_show_names: Maximum number of samples names allowed 
    to be displayed in the plot. If sampels in the study exceeds this value,
    only the group labels will be shown.

    :return pd.DataFrame: Df with the events from all types that pass the
    NA filter. Will be the input for the PCA function with all event types
    """
    samples_names = list(groups.keys())
    if idx_to_use is not None:
        idx = idx_to_use.split(",")
        try:
            idx = [int(x) for x in idx]
        except ValueError:
            raise ValueError('Please set int values in the "--idx_to_use" argument')
        assert len(samples_names) == len(idx), "Number of samples in the groups({}) must match number of " \
                                               "indexes given({})".format(len(samples_names), len(idx))
    inc_level_col = "IncLevel1"
    events = ["SE", "MXE", "A3SS", "A5SS", "RI"]
    event_type_map = {"SE": "ES"}
    results_dict = {}

    with open(files) as f:
        file_list = f.read().splitlines()
    f.close()

    for ev in events:
        print("Processing {} events ".format(event_type_map.get(ev, ev)))
        ev_file = [e for e in file_list if ev in os.path.basename(e)][0]
        ev_dict = {}
        assert os.path.isfile(ev_file), "{} is not a valid file".format(ev_file)
        f = open(ev_file)
        header = f.readline().split("\t")
        assert inc_level_col in header, "IncLevel1 column (PSI quantification) should be present in " \
                                        "rMATS {} file.".format(ev_file)
        idx_target_column = header.index(inc_level_col)
        next(f)
        for line in f:
            l = line.split("\t")
            psi = l[idx_target_column]
            if idx_to_use is not None:
                _psi_val = psi.split(",")
                try:
                    psi = ','.join([_psi_val[i] for i in idx])
                except IndexError:
                    raise IndexError('Some indexes provided are greater than number of samples({})'.
                                     format(len(_psi_val)))

            if any(v for v in psi.split(",") if v != "NA"):  # Keep only events with at least 1 non NA
                ev_dict["{}_{}".format(event_type_map.get(ev, ev), l[0])] = psi
        f.close()
        results_dict[ev] = ev_dict

    list_dfs = []
    for ev_type, psi_dict in results_dict.items():
        df = pd.DataFrame.from_dict(psi_dict, orient="index", columns=["psis"])
        df[samples_names] = df['psis'].str.split(",", expand=True).replace("NA", np.nan)
        del df['psis']
        df = df.dropna(axis=0, thresh=len(df.columns) - allowed_nas).apply(pd.to_numeric, errors='coerce')

        list_dfs.append(df)
        do_PCA(df, allowed_nas, groups, paired, outbasename, n_to_show_names, ev_type, "rMATS")
    concat_df = pd.concat(list_dfs)
    return concat_df


def _extract_majiq_binary_decisions(row: pd.Series):
    """
    Extracts PSI level of inclusion group
    of simple LSV type
    :param lsv_type:
    :param lsv_type_str:
    :param dPSIs:
    :param probabilities_dPSIs:
    :return:
    """
    output = []
    # Pure RI events do not need to know
    # source of target info, because its
    # inclusion on the 2nd group vs 1st group
    # is given in the last jx of the LSV
    if row.lsv_type == "RI":
        for name, psis in row.iteritems():
            if name not in ['lsv_id', 'lsv_type']:

                if len(psis.split(";")) == 2:
                    output.append(round(float(psis.split(";")[-1]), 5))

                else:
                    print("Problem in IR event in an LSV.")
                    exit(1)

    # binary dPSI orientation of 2nd group vs 1st group
    # is extracted based on source|target tags for ES|ALT3SS|ALT5SS
    #################################
    # source exon - most left junction
    # refers to the inclusion
    ################################
    if row.lsv_id.split(":")[1] == "s":
        for name, psis in row.iteritems():
            if id not in ['lsv_id', 'lsv_type']:

                # ALTSS inclusion form is always the shortest transcript
                if len(psis.split(";")) == 2:
                    if row.lsv_type in ['ES', 'A5SS']:
                        inc_idx = 0
                        output.append(round(float(psis.split(";")[inc_idx]), 5))

                    elif row.lsv_type == "A3SS":
                        inc_idx = 1
                        output.append(round(float(psis.split(";")[inc_idx]), 5))

                # if multiple inclusion jx, do average
                elif len(psis.split(";")) > 2:

                    if row.lsv_type in ['ES', 'A5SS']:
                        output.append(round(np.mean([float(x) for x in psis.split(";")[:-1]]), 5))

                    elif row.lsv_type == "A3SS":
                        output.append(round(np.mean([float(x) for x in psis.split(";")[1:]]), 5))

    ######################
    # target exon - most left junction
    # refers to the skipping
    ######################
    elif row.lsv_id.split(":")[1] == "t":
        for name, psis in row.iteritems():
            if name not in ['lsv_id', 'lsv_type']:
                if row.lsv_type in ['A3SS', 'A5SS']:
                    print("SHIT. AltSS when main exon is target(t). {}".format(row.lsv_type))
                    exit(1)

                # binary dPSI orientation of 2nd group vs 1st group
                # is extracted based on source|target tags
                elif len(psis.split(";")) == 2:
                    inc_idx = 1
                    output.append(round(float(psis.split(";")[inc_idx]), 5))

                # if multiple inclusion jx, do average
                elif len(psis.split(";")) > 2:
                    output.append(round(np.mean([float(x) for x in psis.split(";")[1:]]), 5))

    return output


def _get_informative_junction(row: pd.Series) -> pd.Series:
    """
    Extracts index of the junction with the greatest
    variance (if complex event), otherwise extracts
    the PSI level for the inclusion group

    :param pd.Series row: PSI quantifications for a given LSV

    :return pd.Series: Junction from LSV with greatest variance
    """
    # if complex event, extract jx with greatest variance
    if ";" in row.lsv_type:

        junctions = row.str.split(";", expand=True).apply(pd.to_numeric, errors='coerce').drop('lsv_type')
        idx_top_variance = junctions.var().idxmax()

        try:
            top_jx = junctions.iloc[:, idx_top_variance].copy()
            top_jx['lsv_id'] = row.lsv_id
            top_jx['lsv_type'] = "Complex"
            return top_jx.reindex(index = ['lsv_id', 'lsv_type'] + [x for x in list(top_jx.index)
                                                                    if x not in ['lsv_id', 'lsv_type']])

        except ValueError:
            # If not possible to calculate variance (e.g. Only 1 sample with no NaN values)
            return

    else:
        out = _extract_majiq_binary_decisions(row)
        return pd.Series([row.lsv_id, row.lsv_type] + out, index=row.index)


def process_majiq(files: list, allowed_nas: int, groups: dict) -> pd.DataFrame:
    """
    Reads files generated by majiq psi, extracts the PSI
    for the best junction of a given LSV (highest variance
    across all the samples) and performs PCA analysis
    on the samples assuming the sample order represented
    in the `groups` dict.

    :param str files: File listing majiq psi files
    :param int allowed_nas: Number of samples with NAs (unquantifiable
    PSIs) per event allowed, so that event is kept for PCA analysis
    :param dict groups: Dictionary with info about each sample
    and the group they represent

    :return pd.DataFrame: Df of Majiq junctions that pass the NA filter
    """

    psi_col = ["E(PSI) per LSV junction"]
    lsv_id_col = "LSV ID"
    lsv_types = ["A5SS", "A3SS", "ES"]
    psi_dict = {}
    with open(files) as f:
        file_list = f.read().splitlines()
    f.close()
    assert len(file_list) == len(groups.keys()), "Number of majiq files must be the same as the number" \
                                                 " of different samples represented in the 1st column" \
                                                 " of groups file."
    colnames = []
    list_samples_psis = []
    print("Files and sample match:")
    # Per sample dict of PSI for each LSV
    for i, file in enumerate(file_list):
        _df = pd.read_csv(file, sep="\t", low_memory=False)
        _df['type'] = _df[lsv_types].apply(get_relevant_type, axis=1).apply(lambda x: ';'.join(x))
        _df['type'] = np.where(_df['IR coords'].isnull(), _df.type, _df.type + ";RI")

        _d = _df.set_index(lsv_id_col)[['type'] + psi_col].T.to_dict('list')

        list_samples_psis.append(_d)
        colnames.append(list(groups.keys())[i])
        print("{} {}".format(os.path.basename(file), list(groups.keys())[i]))

    # Join dicts into 1 single df
    for i, d in enumerate(list_samples_psis):
        for lsv_id, lsv_psis in d.items():
            if lsv_id in psi_dict.keys():
                _update = psi_dict[lsv_id]
                _update[i] = lsv_psis
                psi_dict[lsv_id] = _update
            else:
                _new_val = [None] * len(list_samples_psis)

                _new_val[i] = lsv_psis
                psi_dict[lsv_id] = _new_val

    df = pd.DataFrame.from_dict(psi_dict, orient="index", columns=colnames)

    def lsv_type_to_col(row: pd.Series):
        """
        Put event type to a single column
        :param pd.Series row: Single row
        """
        _type = ""
        out = []
        for col in list(row):
            if not _type:
                if col:
                    _type = col[0]
            out.append(col[1] if col else col)

        return pd.Series([_type] + out)

    df = df.apply(lsv_type_to_col, axis=1)
    df.columns = ['lsv_type'] + colnames
    df = df.rename_axis('lsv_id').reset_index()
    df = df.dropna(axis=0, thresh=len(df.columns) - allowed_nas)

    print("Selecting informative junction per LSV")
    tqdm.pandas()
    df_ = df.progress_apply(_get_informative_junction, axis=1, result_type='broadcast').dropna(how='all')
    return pd.DataFrame(df_)


def do_PCA(df: pd.DataFrame, allowed_nas: int, groups: dict,
           paired: bool, outbasename: str, n_to_show_names: int,
           name: str, extra: str = None,):
    """
    PCA analysis from PSIs quantifications based on the 2000 events with
    greatest variance. Additionally, plots of principal components based
    on the defined groups are drawn

    :param pd.DataFrame df: Ready df with per-event PSIs to calculate
    principal components
    :param int allowed_nas: Allowed NAs. If > 0, imputations will be
    performed based on the row mean (PCA doesn't accept missing values)
    :param dict groups: Dictionary with info about each sample
    and the group they represent
    :param bool paired: Whether samples of each group are paired (if so,
    order of samples per group must be preserved)
    :param str outbasename: Output basename
    :param int n_to_show_names: Maximum number of samples names allowed 
    to be displayed in the plot. If sampels in the study exceeds this value,
    only the group labels will be shown.
    :param str name: str to add to output (e.g. software name)
    :param str extra: Extra str to add (e.g. rMATS event type)

    that their names appear in the plot. Default: `10`. If df has more 
    than this number of samples, names won't be displayed (just the groups)

    """
    print("Number of {} events that will be used in the PCA: {}".format(name, df.shape[0]))

    if name == "majiq":
        df = df[df.lsv_type != "Complex"]
        print("Events used after removing complex majiq lsvs: {}".format(df.shape[0]))
        df = df.set_index(['lsv_id', 'lsv_type'])

    if allowed_nas > 0:
        print("Doing imputation of missing values")
        df = df.apply(lambda x: x.fillna(x.median()), axis=1)

    df = df.loc[df.var(axis=1).nlargest(2000).index, ]
    pca = PCA(n_components=3)
    PCs = pca.fit_transform(df.T)
    print("Amount of variance that the first PCs contain for {} data: {}".format(name, pca.explained_variance_ratio_))

    PC_df = pd.DataFrame(data=PCs,
                         columns=['PC1', 'PC2', 'PC3'],
                         index=df.T.index)

    if extra is not None:
        output = "{}_{}_{}_PCs.tsv".format(outbasename, extra, name)
    else:
        output = "{}_{}_PCs.tsv".format(outbasename, name)
    PC_df.to_csv(output, sep="\t")

    cols_groups_df = ['Group', 'Ind'] if paired else ['Group']
    groups_df = pd.DataFrame.from_dict(groups, orient='index', columns=cols_groups_df)

    PC_df = PC_df.merge(groups_df,
                        left_index=True,
                        right_index=True).rename_axis('Sample').reset_index()

    perc_PC1 = round(pca.explained_variance_ratio_[0] * 100, 2)
    perc_PC2 = round(pca.explained_variance_ratio_[1] * 100, 2)
    perc_PC3 = round(pca.explained_variance_ratio_[2] * 100, 2)

    for pc_pair in [("PC1", "PC2", perc_PC1, perc_PC2), ("PC2", "PC3", perc_PC2, perc_PC3)]:
        if paired:
            p1 = p9.ggplot(PC_df, p9.aes(x=pc_pair[0], y=pc_pair[1], fill="Group", shape='Ind'))
        
        elif PC_df.shape[0] <= n_to_show_names:
            p1 = p9.ggplot(PC_df, p9.aes(x=pc_pair[0], y=pc_pair[1], fill="Group", shape='Sample'))

        else:
            p1 = p9.ggplot(PC_df, p9.aes(x=pc_pair[0], y=pc_pair[1], fill="Group"))

        p1 = (p1 + p9.geom_point(color="black", size=6, alpha=0.7, position=p9.position_dodge(width=0.3))
              + p9.xlab('{} ({}%)'.format(pc_pair[0], pc_pair[2]))
              + p9.ylab('{} ({}%)'.format(pc_pair[1], pc_pair[3])))

        if extra is not None:
            output = "{}_{}_{}_{}vs{}.pdf".format(outbasename, extra, name, pc_pair[0], pc_pair[1])
        else:
            output = "{}_{}_{}vs{}.pdf".format(outbasename, name, pc_pair[0], pc_pair[1])
        p1.save(output, verbose=False)


def draw_heatmap(df: pd.DataFrame,
                 allowed_nas: int,
                 groups: dict,
                 tool: str,
                 outbasename: str):
    """
    :param pd.DataFrame df: PSI df for the subset of target genes
    :param dict groups:
    :param str tool:
    :param str outbasename:

    """

    to_rename = {}
    for k, v in groups.items():
        to_rename[k] = v[0] + "_" + k
    df = df.rename(columns=to_rename)

    if allowed_nas > 0:
        print("Doing imputation of missing values")
        df = df.apply(lambda x: x.fillna(x.median()), axis=1)

    cm = sns.clustermap(df,
                        cmap='viridis',
                        yticklabels=False,
                        vmin=0,
                        vmax=1,
                        figsize=(8, 6),
                        cbar_kws={'label': 'Inclusion levels'},
                        z_score=0)

    # Aesthetics of the clustergrid
    cm.ax_row_dendrogram.set_visible(False)
    cm.ax_col_dendrogram.set_visible(True)
    cm.cax.set_visible(True)
    ax = cm.ax_heatmap

    ax.set_xticklabels(ax.get_xticklabels(), rotation=80)

    # Add vertical lines
    ax.vlines(range(0, df.shape[1]), ymin=0, ymax=df.shape[0], colors='black', linewidths=0.5)
    ax.axhline(y=0, color='k', linewidth=3)
    ax.axhline(y=df.shape[0], color='k', linewidth=3)
    ax.axvline(x=0, color='k', linewidth=3)
    ax.axvline(x=df.shape[1], color='k', linewidth=3)

    plt.savefig(outbasename + "_" + tool + "_heatmap.pdf", bbox_inches='tight', pad_inches=0)
    plt.close()


def main():
    parser = argparse.ArgumentParser(description='Script to generate PCAs/heatmaps from inclusion levels '
                                                 'of splicing events quantified by different tools.')
    parser.add_argument(dest='tool', choices=("rmats", "majiq", "vastools"), help="Generate samples PCAs from "
                                                                                  "quantifications of given tool")
    parser.add_argument('--groups', required=True, help='2 (or 3) column file with the sample name (1st col) and group'
                                                        ' it belongs (2nd col). if tool == rmats", order of the samples'
                                                        ' in the file should be the same as provided to "--b1" when '
                                                        ' running rMATS. If tool == majiq, order of the samples'
                                                        ' should be the same as the order of files in the "majiq_files"'
                                                        ' argument. 3rd column, if set, should refer to the individual'
                                                        ' name when samples from different groups are paired (--paired'
                                                        ' flag set in the script).')
    parser.add_argument('--NAs_allowed', type=int, default=0, help="Number of samples per event allowed to have "
                                                                   "NA values, so that event is considered when "
                                                                   "calculating principal components. If higher "
                                                                   "than 0, imputation of missing values will be "
                                                                   "performed, based on the median PSI for that "
                                                                   "event. Default: 0")
    parser.add_argument('--paired', action='store_true', help="Whether samples in groups file are paired. If so, "
                                                              "3rd column in groups argument must be set providing "
                                                              "information about each individual")
    parser.add_argument('-o', '--outbasename', help='Basename to write output files')
    parser.add_argument('--n_to_show_names', type=int, default=10, help="Maximum number of sample names to be "
                                                                        "displayed in the plot. If study has more "
                                                                        "than this number of samples, their names "
                                                                        "won't be shown, just the groups which "
                                                                        "they belong. Default: `10`.")
    parser.add_argument('--gene_list', help="Plot heatmap of inclusion levels given a specific group of genes.")

    rmats = parser.add_argument_group('rMATS related arguments')
    rmats.add_argument('--rmats_files', help='Events files produced by rMATS ([ev_type].MATS.[JC|JCEC].txt, one per '
                                             'line. These files should represent a rMATS run where no statistical '
                                             'comparison was performed ("--statoff" and "--b2" not set)')
    rmats.add_argument('--idx_to_use', type=str, help="If samples passed in the groups are a subset of all samples"
                                                      " used in the '--b1' argument in rMATS, set the indexes in the"
                                                      " rMATS files (IncLevel column) that match those samples. "
                                                      " (Indexes start from 0 and are split split by ',')")

    vasttools = parser.add_argument_group('vast-tools related arguments')
    vasttools.add_argument('--tidy', help='Inclusion table produced by vast-tools tidy.')

    majiq = parser.add_argument_group('MAJIQ related arguments')
    majiq.add_argument('--majiq_files', help="LSV quantifications generated by MAJIQ PSI, one sample per line.")
    args = parser.parse_args()

    groups_dict = {}
    with open(args.groups) as f:
        for line in f:
            l = line.rstrip().split("\t")
            _sample = l[0]
            _grp = l[1]
            groups_dict[_sample] = [_grp, l[2]] if args.paired else [_grp]
    f.close()

    if args.tool == "rmats":
        assert args.rmats_files is not None, "When producing PCA from rMATS, please set its group arguments " \
                                             "accordingly. "
        df = process_rMATS(args.rmats_files, args.NAs_allowed, groups_dict, args.paired, args.idx_to_use,
                           args.outbasename, args.n_to_show_names)
        print(df)
        print(df.shape)
        exit(1)
        do_PCA(df, args.NAs_allowed, groups_dict, args.paired,
               args.outbasename, args.n_to_show_names, "all", "rMATS")

    elif args.tool == "vastools":
        assert args.tidy is not None, "When producing PCA from vast-tools, please set its group arguments accordingly."
        df = process_vasttools(args.tidy, args.NAs_allowed, groups_dict)
        do_PCA(df, args.NAs_allowed, groups_dict, args.paired, args.outbasename,
               args.n_to_show_names, "vast-tools")

    elif args.tool == "majiq":
        assert args.majiq_files is not None, "When producing PCA from MAJIQ, please set its group arguments " \
                                             "accordingly. "
        # df = process_majiq(args.majiq_files, args.NAs_allowed, groups_dict)
        # do_PCA(df, args.NAs_allowed, groups_dict, args.paired, args.outbasename,
        #         args.n_to_show_names, "majiq")

        if args.gene_list:
            genes = [line.rstrip() for line in open(args.gene_list, 'r') if not line.startswith("#")]
            # df[['gene_id', 'tag', 'exon_coord']] = df.lsv_id.str.split(":", expand=True)
            # df['gene_id'] = df.gene_id.str.split('.').str[0]
            # df = df[df.gene_id.isin(genes)]
            # df.drop(columns=['gene_id', 'tag', 'exon_coord'], inplace=True)
            # df = df.set_index(['lsv_id', 'lsv_type'])
            df = pd.read_csv("TEST_DF.csv")
            draw_heatmap(df, args.NAs_allowed, groups_dict, "majiq_withComplexLSVs", args.outbasename)
            df = df[df.index.get_level_values('lsv_type') != "Complex"]
            draw_heatmap(df, args.NAs_allowed, groups_dict, "majiq", args.outbasename)


if __name__ == "__main__":
    main()
